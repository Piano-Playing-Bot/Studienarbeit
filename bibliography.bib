@online{.AnalogWriteArduinoReference.,
  title = {{{analogWrite}}() - {{Arduino Reference}}},
  url = {https://www.arduino.cc/reference/en/language/functions/analog-io/analogwrite/},
  urldate = {2024-02-08},
  file = {C:\Users\Administrator\Zotero\storage\WJ685Z9E\analogwrite.html}
}

@online{.PortugueseMusicianBreaks.17,
  title = {Portuguese Musician Breaks Record for Astonishingly Fast Piano Key Hitting},
  date = {2017-06-07T10:05:45.0000000},
  url = {https://www.guinnessworldrecords.com/news/2017/6/portuguese-musician-breaks-record-for-astonishingly-fast-piano-key-hitting-475329},
  urldate = {2024-02-08},
  abstract = {Portuguese-American musician Antonio Domingos has proved he has the fastest fingers in the business, after smashing the record for the most piano key hits in one minute.},
  langid = {british},
  organization = {{Guinness World Records}},
  file = {C:\Users\Administrator\Zotero\storage\KS8TIWRL\portuguese-musician-breaks-record-for-astonishingly-fast-piano-key-hitting-475329.html}
}

@online{.ShiftRegisterPWMLibraryArduinoReference.,
  title = {{{ShiftRegister-PWM-Library}} - {{Arduino Reference}}},
  url = {https://www.arduino.cc/reference/en/libraries/shiftregister-pwm-library/},
  urldate = {2024-01-25},
  file = {C:\Users\Administrator\Zotero\storage\RHKUU57C\shiftregister-pwm-library.html}
}

@online{ard.ArduinoLanguageReference.,
  title = {Arduino {{Language Reference}}},
  author = {Arduino},
  url = {https://www.arduino.cc/reference/en/},
  urldate = {2023-10-09},
  file = {C:\Users\Administrator\Zotero\storage\XF2XMQ94\en.html}
}

@online{ard.ArduinoLibrariesReference.,
  title = {Arduino {{Libraries Reference}}},
  author = {Arduino},
  url = {https://www.arduino.cc/reference/en/libraries/},
  urldate = {2023-10-09},
  file = {C:\Users\Administrator\Zotero\storage\BDE4ULEP\libraries.html}
}

@online{ard.GettingStartedArduino.,
  title = {Getting {{Started}} with {{Arduino}} | {{Arduino Documentation}}},
  author = {Arduino},
  url = {https://docs.arduino.cc/learn/starting-guide/getting-started-arduino},
  urldate = {2024-01-01},
  abstract = {An introduction to hardware, software tools, and the Arduino API.},
  file = {C:\Users\Administrator\Zotero\storage\7Y24QCHY\getting-started-arduino.html}
}

@inproceedings{bad.WorkingPrincipleArduino.14,
  title = {The Working Principle of an {{Arduino}}},
  booktitle = {2014 11th {{International Conference}} on {{Electronics}}, {{Computer}} and {{Computation}} ({{ICECCO}})},
  author = {Badamasi, Yusuf Abdullahi},
  date = {2014-09},
  pages = {1--4},
  doi = {10.1109/ICECCO.2014.6997578},
  url = {https://ieeexplore.ieee.org/abstract/document/6997578?casa_token=5FGGwxoDCbAAAAAA%3A2Kt35-pw_cq4RCF75T0h43ZihPZ5vtPwKCtEBx5h9f7IZQGhBw6AmoHxd3P0TD6kypA3a5RZ7JFA},
  urldate = {2023-10-09},
  abstract = {In this paper, we analyze the working principle of an arduino. These days many people try to use the arduino because it makes things easier due to the simplified version of C++ and the already made Arduino microcontroller(atmega328 microcontroller [1]) that you can programme, erase and reprogramme at any given time. In this paper we will discuss the hardware components used in the arduino board, the software used to programme it (Arduino board) with the guide on how to write and construct your own projects, and a couple of examples of an arduino project, This will give you the overall view of an arduino uno, that after reading this paper you will get the basic concept and use of an arduino uno.},
  eventtitle = {2014 11th {{International Conference}} on {{Electronics}}, {{Computer}} and {{Computation}} ({{ICECCO}})},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\2XYQJIMC\\Badamasi - 2014 - The working principle of an Arduino.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\KJ5QQMSE\\6997578.html}
}

@article{brcf.OpticalMusicRecognition.19,
  title = {From {{Optical Music Recognition}} to {{Handwritten Music Recognition}}: {{A}} Baseline},
  shorttitle = {From {{Optical Music Recognition}} to {{Handwritten Music Recognition}}},
  author = {Baró, Arnau and Riba, Pau and Calvo-Zaragoza, Jorge and Fornés, Alicia},
  date = {2019-05-15},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  volume = {123},
  pages = {1--8},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2019.02.029},
  url = {https://www.sciencedirect.com/science/article/pii/S0167865518303386},
  urldate = {2023-10-09},
  abstract = {Optical Music Recognition (OMR) is the branch of document image analysis that aims to convert images of musical scores into a computer-readable format. Despite decades of research, the recognition of handwritten music scores, concretely the Western notation, is still an open problem, and the few existing works only focus on a specific stage of OMR. In this work, we propose a full Handwritten Music Recognition (HMR) system based on Convolutional Recurrent Neural Networks, data augmentation and transfer learning, that can serve as a baseline for the research community.},
  keywords = {Deep neural networks,Document image analysis and recognition,Handwritten music recognition,LSTM,Optical music recognition},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\UFQ9V2QR\\Baró et al. - 2019 - From Optical Music Recognition to Handwritten Musi.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\M9WMJKJ6\\S0167865518303386.html}
}

@article{bre.ExploringImmediateMode.22,
  title = {Exploring the {{Immediate Mode GUI Concept}} for {{Graphical User Interfaces}} in {{Mixed Reality Applications}}},
  author = {Brendel, Felix},
  date = {2022},
  journaltitle = {Gesellschaft für Informatik e.V.},
  pages = {12},
  doi = {10.18420/vrar2022_1678},
  url = {https://dl.gi.de/handle/20.500.12116/40168},
  urldate = {2024-01-11},
  abstract = {The state of the art GUI is based on widget lifetimes that are bound to objects or handles. This common approach which is called retained mode GUI has a few drawbacks. The application data has to be kept in sync with the UI manually. An alternative approach is the immediate mode GUI which differs from retained mode in that no UI widget has a lifetime and does not require updates. In every frame, the application instructs the immediate mode system what to display on the screen. Immediate mode GUI systems are already widely used in games and game tooling. In this work, the possibilities of an immediate mode system in the context of mixed reality are explored.},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\XHX7LGQZ\Brendel - Exploring the Immediate Mode GUI Concept for Graph.pdf}
}

@inproceedings{brf.RecognitionCompoundMusic.16,
  title = {Towards the {{Recognition}} of {{Compound Music Notes}} in {{Handwritten Music Scores}}},
  booktitle = {2016 15th {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}} ({{ICFHR}})},
  author = {Baró, Arnau and Riba, Pau and Fornés, Alicia},
  date = {2016-10},
  pages = {465--470},
  issn = {2167-6445},
  doi = {10.1109/ICFHR.2016.0092},
  url = {https://ieeexplore.ieee.org/abstract/document/7814108?casa_token=0t5S-NtZEYMAAAAA%3AU0G8OisDLl5-tXydyOuRUkvBlj5nMVAAiYbb7NdswRVt0vn-soeUjTX2aHFTQSWzhLo-PO7DHCOR},
  urldate = {2023-10-09},
  abstract = {The recognition of handwritten music scores still remains an open problem. The existing approaches can only deal with very simple handwritten scores mainly because of the variability in the handwriting style and the variability in the composition of groups of music notes (i.e. compound music notes). In this work we focus on this second problem and propose a method based on perceptual grouping for the recognition of compound music notes. Our method has been tested using several handwritten music scores of the CVC-MUSCIMA database and compared with a commercial Optical Music Recognition (OMR) software. Given that our method is learning-free, the obtained results are promising.},
  eventtitle = {2016 15th {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}} ({{ICFHR}})},
  file = {C:\Users\Administrator\Zotero\storage\UJAC9WB3\Baró et al. - 2016 - Towards the Recognition of Compound Music Notes in.pdf}
}

@software{bsc.Bsch2734SpiShiftRegisterChain.22,
  title = {Bsch2734/{{SpiShiftRegisterChain}}},
  author = {{bsch2734}},
  date = {2022-04-26T02:35:11Z},
  origdate = {2022-04-26T02:25:04Z},
  url = {https://github.com/bsch2734/SpiShiftRegisterChain},
  urldate = {2024-01-25},
  abstract = {A simple Arduino library for controlling any length of chained 595 style shift registers over the built in SPI bus},
  keywords = {arduino,arduino-library,library,shift-register}
}

@article{cjp.UnderstandingOpticalMusic.20,
  title = {Understanding {{Optical Music Recognition}}},
  author = {Calvo-Zaragoza, Jorge and Jr., Jan Hajič and Pacha, Alexander},
  date = {2020-07-22},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  number = {4},
  pages = {77:1--77:35},
  issn = {0360-0300},
  doi = {10.1145/3397499},
  url = {https://dl.acm.org/doi/10.1145/3397499},
  urldate = {2023-10-09},
  abstract = {For over 50 years, researchers have been trying to teach computers to read music notation, referred to as Optical Music Recognition (OMR). However, this field is still difficult to access for new researchers, especially those without a significant musical background: Few introductory materials are available, and, furthermore, the field has struggled with defining itself and building a shared terminology. In this work, we address these shortcomings by (1) providing a robust definition of OMR and its relationship to related fields, (2) analyzing how OMR inverts the music encoding process to recover the musical notation and the musical semantics from documents, and (3) proposing a taxonomy of OMR, with most notably a novel taxonomy of applications. Additionally, we discuss how deep learning affects modern OMR research, as opposed to the traditional pipeline. Based on this work, the reader should be able to attain a basic understanding of OMR: its objectives, its inherent structure, its relationship to other fields, the state of the art, and the research opportunities it affords.},
  keywords = {music notation,music scores,Optical music recognition},
  file = {C:\Users\Administrator\Zotero\storage\I4BYU4K6\Calvo-Zaragoza et al. - 2020 - Understanding Optical Music Recognition.pdf}
}

@book{geo.VisualPerceptionMusic.04,
  title = {Visual {{Perception}} of {{Music Notation}}: {{On-Line}} and {{Off-Line Recognition}}},
  author = {George, Susan Ella},
  date = {2004},
  publisher = {{IRM Press, Idea Group Inc.}},
  abstract = {In this text for undergraduate and graduate students in technology-related music programs, George (U. of South Australia) and contributors describe computer recognition of music notation, including off-line music processing, handwritten music recognition, and lyric recognition. Chapter topics include staff detection and removal, a method of off-line optical music sheet recognition, using wavelets to deal with superimposed objects, pen-based input for online hand-written notation, lyric recognition and Christian music, and multilingual lyric modeling and management. In a section covering music description and its applications, contributors examine constructing emotional landscapes with music and modeling music notations in the Internet multimedia age. The final chapter describes evaluation in the visual perception of music},
  isbn = {1-59140-298-0},
  langid = {english},
  pagetotal = {357}
}

@inproceedings{ggla.PitchEstimationMusical.14,
  title = {Pitch Estimation for Musical Note Recognition Using {{Artificial Neural Networks}}},
  booktitle = {2014 {{International Conference}} on {{Electronics}}, {{Communications}} and {{Computers}} ({{CONIELECOMP}})},
  author = {Guerrero-Turrubiates, Jose de Jesus and Gonzalez-Reyna, Sheila Esmeralda and Ledesma-Orozco, Sergio Eduardo and Avina-Cervantes, Juan Gabriel},
  date = {2014-02},
  pages = {53--58},
  doi = {10.1109/CONIELECOMP.2014.6808567},
  url = {https://ieeexplore.ieee.org/abstract/document/6808567?casa_token=RjffFklm15UAAAAA%3A132QZxiw77tfT5m8aOonYObRwptiChUK1LRcgGoz0A5XTG840g_JhqgkDqR0UgaUEaE0FXJC0MP9},
  urldate = {2023-10-09},
  abstract = {Pitch estimation has increased its importance due to the wide variety of applications in different fields, e.g. speech and voice recognition, music transcription, to name a few. Musical signals may contain noise and distortion, therefore pitch detection results can be erroneous. In this paper, a musical note recognition system based on harmonic modification and Artificial Neural Network (ANN) is proposed. At first, downsampling is applied to convert the signal from 44,100 Hz sampling rate to 2,100 Hz. Fast Fourier Transform (FFT) is used to obtain the signal spectrum; Harmonic Product Spectrum (HPS) algorithm is implemented to enhance the fundamental frequency amplitude. Then a dimensionality reduction method based on variances, is used to extract relevant information from the input signal. In the present work, audio signals were taken from a proprietary database that was constructed using an electric guitar as audio source. The classification is performed by a feed-forward neural network or Multi-Layer Perceptron (MLP). Experimental results present accurate classification with few processing of the input signal. Besides the proposed approach presents enough robustness to classify musical notes coming from different musical instruments.},
  eventtitle = {2014 {{International Conference}} on {{Electronics}}, {{Communications}} and {{Computers}} ({{CONIELECOMP}})},
  file = {C:\Users\Administrator\Zotero\storage\RZPCF5Z3\6808567.html}
}

@inproceedings{ggla.PitchEstimationMusical.14a,
  title = {Pitch Estimation for Musical Note Recognition Using {{Artificial Neural Networks}}},
  booktitle = {2014 {{International Conference}} on {{Electronics}}, {{Communications}} and {{Computers}} ({{CONIELECOMP}})},
  author = {Guerrero-Turrubiates, Jose de Jesus and Gonzalez-Reyna, Sheila Esmeralda and Ledesma-Orozco, Sergio Eduardo and Avina-Cervantes, Juan Gabriel},
  date = {2014-02},
  pages = {53--58},
  doi = {10.1109/CONIELECOMP.2014.6808567},
  url = {https://ieeexplore.ieee.org/document/6808567},
  urldate = {2023-10-09},
  abstract = {Pitch estimation has increased its importance due to the wide variety of applications in different fields, e.g. speech and voice recognition, music transcription, to name a few. Musical signals may contain noise and distortion, therefore pitch detection results can be erroneous. In this paper, a musical note recognition system based on harmonic modification and Artificial Neural Network (ANN) is proposed. At first, downsampling is applied to convert the signal from 44,100 Hz sampling rate to 2,100 Hz. Fast Fourier Transform (FFT) is used to obtain the signal spectrum; Harmonic Product Spectrum (HPS) algorithm is implemented to enhance the fundamental frequency amplitude. Then a dimensionality reduction method based on variances, is used to extract relevant information from the input signal. In the present work, audio signals were taken from a proprietary database that was constructed using an electric guitar as audio source. The classification is performed by a feed-forward neural network or Multi-Layer Perceptron (MLP). Experimental results present accurate classification with few processing of the input signal. Besides the proposed approach presents enough robustness to classify musical notes coming from different musical instruments.},
  eventtitle = {2014 {{International Conference}} on {{Electronics}}, {{Communications}} and {{Computers}} ({{CONIELECOMP}})},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\URX2TQFH\\Guerrero-Turrubiates et al. - 2014 - Pitch estimation for musical note recognition usin.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\XJXPLBP8\\6808567.html}
}

@article{kuh.RealTimePitchRecognition.90,
  title = {A {{Real-Time Pitch Recognition Algorithm}} for {{Music Applications}}},
  author = {Kuhn, William B.},
  date = {1990},
  journaltitle = {Computer Music Journal},
  volume = {14},
  number = {3},
  eprint = {3679960},
  eprinttype = {jstor},
  pages = {60--71},
  publisher = {{The MIT Press}},
  issn = {01489267, 15315169},
  doi = {10.2307/3679960},
  url = {http://www.jstor.org/stable/3679960},
  urldate = {2023-10-09},
  file = {C:\Users\Administrator\Zotero\storage\SQ6ZGGQU\Kuhn - 1990 - A Real-Time Pitch Recognition Algorithm for Music .pdf}
}

@incollection{lt.WhatMIDI.17,
  title = {What Is {{MIDI}}},
  booktitle = {{{MIDI For The Professional}}},
  author = {Lehrman, Paul D and Tully, Tim},
  date = {2017},
  pages = {21},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\ZDCER2SN\Lehrman and Tully - 2017 - What is MIDI.pdf}
}

@inproceedings{lzx.RobustMethodMusical.15,
  title = {A {{Robust Method}} for {{Musical Note Recognition}}},
  booktitle = {2015 14th {{International Conference}} on {{Computer-Aided Design}} and {{Computer Graphics}} ({{CAD}}/{{Graphics}})},
  author = {Liu, Xiaoxiang and Zhou, Mi and Xu, Peng},
  date = {2015-08},
  pages = {212--213},
  doi = {10.1109/CADGRAPHICS.2015.34},
  url = {https://ieeexplore.ieee.org/abstract/document/7450422/references#references},
  urldate = {2023-10-09},
  abstract = {Musical note recognition plays a fundamental role in the process of the optical music recognition system. In this paper, we propose a robust method for recognizing notes. The method includes three parts: (1) the description of relationships between primitives by introducing the concept of interaction field, (2) the definition of six hierarchical structure features for analyzing notes structures, (3) the workflow of primitive assembly under the guidance of giving priority to key structure features. To evaluate the performance of our method, we present experimental results on real-life scores and comparisons with two commercial products. Experiment show that our method lead to quite good results, especially for complicated scores.},
  eventtitle = {2015 14th {{International Conference}} on {{Computer-Aided Design}} and {{Computer Graphics}} ({{CAD}}/{{Graphics}})},
  file = {C:\Users\Administrator\Zotero\storage\9CJ8YX9Z\references.html}
}

@article{mc.AutomaticMusicalInstrument.,
  title = {Automatic {{Musical Instrument}} and {{Note Recognition}}},
  author = {Malheiro, Frederico and Cavaco, Sofia},
  abstract = {We propose a sound recognizer that uses a reduced feature set to identify musical instruments from single notes in sound recordings as well as the note that was played. The recognizer learns a set of spectral features from the data using non-negative matrix factorization.},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\LW7L6REP\Malheiro and Cavaco - Automatic Musical Instrument and Note Recognition.pdf}
}

@standard{mid.CommonRulesMIDICI.20,
  title = {Common {{Rules}} for {{MIDI-CI Profiles}}},
  shorttitle = {M2-100-{{U}}},
  author = {MIDI Association},
  date = {2020-02-20},
  publisher = {{Association of Musical Electronics Industry AMEI, MIDI Manufacturers Association MMA}},
  langid = {english},
  pagetotal = {17},
  version = {1.0},
  file = {C:\Users\Administrator\Zotero\storage\6VAREZTK\2020 - Common Rules for MIDI-CI Profiles.pdf}
}

@book{mid.CompleteMIDIDetailed.96,
  title = {The {{Complete MIDI}} 1.0 {{Detailed Specification}}},
  shorttitle = {{{MIDI}} Version 96.1},
  author = {MIDI Association},
  date = {1996},
  edition = {3rd},
  publisher = {{Association of Musical Electronics Industry AMEI, MIDI Manufacturers Association MMA}},
  langid = {english},
  pagetotal = {334},
  file = {C:\Users\Administrator\Zotero\storage\VT9J5VWU\1996 - The Complete MIDI 1.0 Detailed Specification.pdf}
}

@online{mid.ControlChangeMessages.,
  title = {Control {{Change Messages}} ({{Data Bytes}})},
  author = {MIDI Association},
  url = {https://docs.google.com/viewer?url=https://midi.org/component/edocman/midi-1-0-control-change-messages-data-bytes-2/fdocument?Itemid=9999},
  urldate = {2023-10-10},
  file = {C:\Users\Administrator\Zotero\storage\Y8K2RSTM\Control Change Messages (Data Bytes).pdf}
}

@online{mid.ExpandedMIDIMessages.,
  title = {Expanded {{MIDI}} 1.0 {{Messages List}} ({{Status Bytes}})},
  author = {MIDI Association},
  url = {https://midi.org/specifications-old/item/table-2-expanded-messages-list-status-bytes},
  urldate = {2023-10-10}
}

@standard{mid.MIDICapabilityInquiry.20,
  title = {{{MIDI Capability Inquiry}} ({{MIDI-CI}})},
  shorttitle = {M2-101-{{UM}}},
  author = {MIDI Association},
  date = {2020-02-20},
  publisher = {{Association of Musical Electronics Industry AMEI, MIDI Manufacturers Association MMA}},
  langid = {english},
  pagetotal = {54},
  version = {1.0},
  file = {C:\Users\Administrator\Zotero\storage\4NPUC2VT\2020 - MIDI Capability Inquiry (MIDI-CI).pdf}
}

@standard{mid.MIDISpecificationOverview.20,
  title = {{{MIDI}} 2.0 {{Specification Overview}}},
  shorttitle = {M2-100-{{U}}},
  author = {MIDI Association},
  date = {2020-02-20},
  publisher = {{Association of Musical Electronics Industry AMEI, MIDI Manufacturers Association MMA}},
  langid = {english},
  pagetotal = {5},
  version = {1.0},
  file = {C:\Users\Administrator\Zotero\storage\9JKP29KW\2020 - MIDI 2.0 Specification Overview.pdf}
}

@online{mid.SummaryMIDIMessages.,
  title = {Summary of {{MIDI}} 1.0 {{Messages}}},
  author = {MIDI Association},
  url = {https://midi.org/specifications-old/item/table-1-summary-of-midi-message},
  urldate = {2023-10-10},
  file = {C:\Users\Administrator\Zotero\storage\MGV7Q258\table-1-summary-of-midi-message.html}
}

@standard{mid.UniversalMIDIPacket.20,
  title = {Universal {{MIDI Packet}} ({{UMP}}) {{Format}} and {{MIDI}} 2.0 {{Protocol}}},
  shorttitle = {M2-103-{{UM}}},
  author = {MIDI Association},
  date = {2020-02-20},
  publisher = {{Association of Musical Electronics Industry AMEI, MIDI Manufacturers Association MMA}},
  langid = {english},
  pagetotal = {80},
  version = {1.0},
  file = {C:\Users\Administrator\Zotero\storage\LTPHJU9H\2020 - Universal MIDI Packet (UMP) Format and MIDI 2.0 Pr.pdf}
}

@online{mur.ImmediateModeGraphicalUser.,
  title = {Immediate-{{Mode Graphical User Interfaces}} (2005)},
  author = {Muratori, Casey},
  url = {https://caseymuratori.com/blog_0001},
  urldate = {2023-10-12},
  abstract = {An old makeshift tech video I recorded introducing the concept of “IMGUI” ~—~ the Immediate-Mode Graphical User Interface.},
  langid = {english},
  organization = {{Immediate-Mode Graphical User Interfaces (2005)}},
  file = {C:\Users\Administrator\Zotero\storage\R5BJRIEB\blog_0001.html}
}

@article{rb.RobustAdaptiveOMR.07,
  title = {Robust and Adaptive {{OMR}} System Including {{Fuzzy}} Modeling, Fusion of Musical Rules, and Possible Error Detection},
  author = {Rossant, Florence and Bloch, Isabelle},
  date = {2007-01-01},
  journaltitle = {EURASIP Journal on Advances in Signal Processing},
  shortjournal = {EURASIP J. Adv. Signal Process},
  volume = {2007},
  number = {1},
  pages = {160},
  issn = {1110-8657},
  doi = {10.1155/2007/81541},
  url = {https://dl.acm.org/doi/10.1155/2007/81541},
  urldate = {2023-10-09},
  abstract = {This paper describes a system for optical music recognition (OMR) in case of monophonic typeset scores. After clarifying the difficulties specific to this domain, we propose appropriate solutions at both image analysis level and high-level interpretation. Thus, a recognition and segmentation method is designed, that allows dealing with common printing defects and numerous symbol interconnections. Then, musical rules are modeled and integrated, in order to make a consistent decision. This high-level interpretation step relies on the fuzzy sets and possibility framework, since it allows dealing with symbol variability, flexibility, and imprecision of music rules, and merging all these heterogeneous pieces of information. Other innovative features are the indication of potential errors and the possibility of applying learning procedures, in order to gain in robustness. Experiments conducted on a large data base show that the proposed method constitutes an interesting contribution to OMR.},
  file = {C:\Users\Administrator\Zotero\storage\L88UB7Q8\Rossant and Bloch - 2007 - Robust and adaptive OMR system including Fuzzy mod.pdf}
}

@online{sf.OpticalMusicRecognition.20,
  title = {Optical {{Music Recognition}}: {{State}} of the {{Art}} and {{Major Challenges}}},
  shorttitle = {Optical {{Music Recognition}}},
  author = {Shatri, Elona and Fazekas, György},
  date = {2020-06-14},
  url = {https://arxiv.org/abs/2006.07885v2},
  urldate = {2023-10-09},
  abstract = {Optical Music Recognition (OMR) is concerned with transcribing sheet music into a machine-readable format. The transcribed copy should allow musicians to compose, play and edit music by taking a picture of a music sheet. Complete transcription of sheet music would also enable more efficient archival. OMR facilitates examining sheet music statistically or searching for patterns of notations, thus helping use cases in digital musicology too. Recently, there has been a shift in OMR from using conventional computer vision techniques towards a deep learning approach. In this paper, we review relevant works in OMR, including fundamental methods and significant outcomes, and highlight different stages of the OMR pipeline. These stages often lack standard input and output representation and standardised evaluation. Therefore, comparing different approaches and evaluating the impact of different processing methods can become rather complex. This paper provides recommendations for future work, addressing some of the highlighted issues and represents a position in furthering this important field of research.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {C:\Users\Administrator\Zotero\storage\2WATQUUN\Shatri and Fazekas - 2020 - Optical Music Recognition State of the Art and Ma.pdf}
}

@article{smp.ST1212014.14,
  title = {{{ST}} 12-1:2014 - {{SMPTE Standard}} - {{Time}} and {{Control Code}}},
  shorttitle = {{{ST}} 12-1},
  author = {SMPTE},
  date = {2014-02},
  journaltitle = {ST 12-1:2014},
  pages = {1--41},
  doi = {10.5594/SMPTE.ST12-1.2014},
  url = {https://ieeexplore.ieee.org/document/7291029},
  urldate = {2024-01-11},
  abstract = {This Standard specifies a time and control code for use in television and accompanying audio systems operating at nominal rates1 of 60, 59.94, 50, 48, 47.95, 30, 29.97, 25, 24, and 23.98 frames per second. This standard defines a time address, binary groups, and flag bit structure. The standard also defines a binary group flag assignment, a linear time code transport, and a vertical interval time code transport. — This standard defines primary data transport structures for Linear Time Code (LTC) and Vertical Interval Time Code (VITC). This standard defines the LTC modulation and timing for all video formats. This standard also defines the VITC modulation and location for 525/59.94 and 625/50 analog composite and component systems only. — Note: The digital representation of analog VITC (D-VITC) is specified in SMPTE ST 266 and is defined for 525/59.94 and 625/50 digital component systems only. High Definition formats, such as those documented in SMPTE ST 2048-2, SMPTE ST 274, and SMPTE ST 296, use Ancillary Time Code (ATC) as specified in SMPTE ST 12-2 (formerly SMPTE RP 188) for transport of time code in the digital video data stream. For future implementations of time code for digital Standard Definition formats, the use of ATC rather than D-VITC is encouraged. — NOTE: This 2014 document is a revision of ST 12-1:2008 incorporating changes published as Amendment ST 12-1:2008 AM1:2013. No further changes have been made.},
  eventtitle = {{{ST}} 12-1:2014},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\UV6F5XNP\\2014 - ST 12-12014 - SMPTE Standard - Time and Control C.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\YVSXK38A\\7291029.html}
}

@article{st.AudioCodingRepresentation.98,
  title = {Audio {{Coding}} for {{Representation}} in {{MIDI}} via {{Pitch Detection Using Harmonic Dictionaries}}},
  author = {Sieger, Nicholas J. and Tewfik, Ahmed H.},
  date = {1998-10-01},
  journaltitle = {Journal of VLSI signal processing systems for signal, image and video technology},
  shortjournal = {The Journal of VLSI Signal Processing-Systems for Signal, Image, and Video Technology},
  volume = {20},
  number = {1},
  pages = {45--59},
  issn = {0922-5773},
  doi = {10.1023/A:1008074130468},
  url = {https://doi.org/10.1023/A:1008074130468},
  urldate = {2023-10-09},
  abstract = {The search for a flexible and concise alternate representation for digital musical sound leads to the proposal for the use of the MIDI (Musical Instrument Digital Interface) protocol. The problem becomes one of automating the conversion process from sound to MIDI. This requires processing musical sound and extracting the information necessary to represent the sound as MIDI data. We have conducted studies which have led to algorithms for segmentation of the sound and pitch detection of the individual notes. We describe a novel method for pitch detection using subset selection with dictionaries containing harmonic spectra from samples of musical sounds. Examples demonstrating applicability to monophonic sounds as well as signals with multiple sound sources are given, including detection of objects in a complex background scene.},
  langid = {english},
  keywords = {Audio Code,Frequency Track,Harmonic Spectrum,Musical Sound,Subset Selection},
  file = {C:\Users\Administrator\Zotero\storage\U4U5LHTN\Sieger and Tewfik - 1998 - Audio Coding for Representation in MIDI via Pitch .pdf}
}

@article{tk.EffectiveOptimizationBasedNeural.19,
  title = {An {{Effective Optimization-Based Neural Network}} for {{Musical Note Recognition}}},
  author = {Tamboli, Allabakash Isak and Kokate, Rajendra D.},
  date = {2019-01-01},
  journaltitle = {Journal of Intelligent Systems},
  volume = {28},
  number = {1},
  pages = {173--183},
  publisher = {{De Gruyter}},
  issn = {2191-026X},
  doi = {10.1515/jisys-2017-0038},
  url = {https://www.degruyter.com/document/doi/10.1515/jisys-2017-0038/html},
  urldate = {2023-10-09},
  abstract = {Musical pitch estimation is used to recognize the musical note pitch or the fundamental frequency ( F 0 ) of an audio signal, which can be applied to a preprocessing part of many applications, such as sound separation and musical note transcription. In this work, a method for musical note recognition based on the classification framework has been designed using an optimization-based neural network (OBNN). A broad range of survey and research was reviewed, and all revealed the methods to recognize the musical notes. An OBNN is used here in recognizing musical notes. Similarly, we can progress the effectiveness of musical note recognition using different methodologies. In this document, the most modern investigations related to musical note recognition are effectively analyzed and put in a nutshell to effectively furnish the traits and classifications.},
  langid = {english},
  keywords = {Audio signal,musical notes,neural network,optimization},
  file = {C:\Users\Administrator\Zotero\storage\G2DDBDXH\Tamboli and Kokate - 2019 - An Effective Optimization-Based Neural Network for.pdf}
}

@online{van.MIDITutorialProgrammers.,
  title = {{{MIDI}} Tutorial for Programmers},
  author = {Vandenneucker, Dominique},
  url = {http://www.music-software-development.com/midi-tutorial.html},
  urldate = {2023-11-02},
  abstract = {This MIDI tutorial will help programmers understand the MIDI language},
  file = {C:\Users\Administrator\Zotero\storage\EEI2AN7D\MIDI tutorial for programmers.html}
}

@book{wam.ArduinoRobotics.11,
  title = {Arduino {{Robotics}}},
  author = {Warren, John-David and Adams, Josh and Molle, Harald},
  date = {2011},
  publisher = {{Apress}},
  location = {{Berkeley, CA}},
  doi = {10.1007/978-1-4302-3184-4},
  url = {http://link.springer.com/10.1007/978-1-4302-3184-4},
  urldate = {2023-10-09},
  isbn = {978-1-4302-3183-7 978-1-4302-3184-4},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\2QN7IFC5\Warren et al. - 2011 - Arduino Robotics.pdf}
}

@inreference{wik.Mikrocontroller.23,
  title = {Mikrocontroller},
  booktitle = {Wikipedia},
  author = {Wikipedia},
  date = {2023-09-16T20:22:50Z},
  url = {https://de.wikipedia.org/w/index.php?title=Mikrocontroller&oldid=237392385},
  urldate = {2023-10-27},
  abstract = {Als Mikrocontroller (auch µController, µC, MCU oder Einchipmikrorechner) werden Halbleiterchips bezeichnet, die einen Prozessor und zugleich auch Peripheriefunktionen enthalten. In vielen Fällen befindet sich auch der Arbeits- und Programmspeicher teilweise oder komplett auf demselben Chip. Ein Mikrocontroller ist ein Ein-Chip-Computersystem. Für manche Mikrocontroller wird auch der Begriff System-on-a-Chip (SoC) verwendet. Auf modernen Mikrocontrollern finden sich häufig auch komplexe Peripheriefunktionen wie z. B. CAN- (Controller Area Network), LIN- (Local Interconnect Network), USB- (Universal Serial Bus), I²C- (Inter-Integrated Circuit), SPI- (Serial Peripheral Interface), serielle oder Ethernet-Schnittstellen, PDM-Ausgänge, LCD-Controller und -Treiber sowie Analog-Digital-Umsetzer. Einige Mikrocontroller verfügen auch über programmierbare digitale und/oder analoge bzw. hybride Funktionsblöcke.},
  langid = {ngerman},
  annotation = {Page Version ID: 237392385},
  file = {C:\Users\Administrator\Zotero\storage\SYZB5DXH\Mikrocontroller.html}
}

@inproceedings{ykl.MaskMatchingLow.08,
  title = {Mask {{Matching}} for {{Low Resolution Musical Note Recognition}}},
  booktitle = {2008 {{IEEE International Symposium}} on {{Signal Processing}} and {{Information Technology}}},
  author = {Yoo, JaeMyeong and Kim, GiHong and Lee, GueeSang},
  date = {2008-12},
  pages = {223--226},
  issn = {2162-7843},
  doi = {10.1109/ISSPIT.2008.4775718},
  url = {https://ieeexplore.ieee.org/abstract/document/4775718?casa_token=-98YXKClqN8AAAAA%3AiXS8uiwbRywp0_0Z6A3sPyDMDUuduV0V5CFMsAGzz9aaFz29pwVbto9iag6eCWagbClsV45Pb1xq},
  urldate = {2023-10-09},
  abstract = {In this paper, we deal with the recognition of musical scores in a low resolution image captured by the digital camera of a mobile phone. Compared to clean, scanned images, low resolution images contain music scores with severe distortions or irrecoverable discontinuities. Most of existing approaches for scanned music scores cannot be applied to the scores with such damages. In this paper, we present mask based approach to cope with incomplete information in the low resolution image. We mainly focus on musical notes in this paper. After the separation of objects in the score into symbols and musical notes, musical notes are identified by mask matching. For the head of a musical note, a 2D head mask is employed while 1D mask is used to extract features for tails. Experimental results show the effectiveness of our approach with the recognition rate of close to 100\% in sample images.},
  eventtitle = {2008 {{IEEE International Symposium}} on {{Signal Processing}} and {{Information Technology}}},
  file = {C\:\\Users\\Administrator\\Zotero\\storage\\MZEH8R3Q\\Yoo et al. - 2008 - Mask Matching for Low Resolution Musical Note Reco.pdf;C\:\\Users\\Administrator\\Zotero\\storage\\HY6Y4L3J\\4775718.html}
}

@article{yr.NewParallelBat.21,
  title = {A New Parallel Bat Algorithm for Musical Note Recognition},
  author = {Younis, Ansam Nazar and Ramo, Fawzia Mahmood},
  date = {2021-02-01},
  journaltitle = {International Journal of Electrical and Computer Engineering (IJECE)},
  shortjournal = {IJECE},
  volume = {11},
  number = {1},
  pages = {558},
  issn = {2722-2578, 2088-8708},
  doi = {10.11591/ijece.v11i1.pp558-566},
  url = {http://ijece.iaescore.com/index.php/IJECE/article/view/21964},
  urldate = {2023-10-09},
  abstract = {Music is a universal language that does not require an interpreter, where feelings and sensitivities are united, regardless of the different peoples and languages, The proposed system consists of two main stages: the process of extracting important properties using the linear discrimination analysis (LDA) This step is carried out after the initial treatment process using various procedures to remove musical lines, The second stage describes the recognition process using the bat algorithm, which is one of the metaheuristic algorithms after modifying the bat algorithm to obtain better discriminating results. The proposed system was supported by parallel implementation using the (developed bat algorithm DBA), which increased the speed of implementation significantly. The method was applied to 1250 different images of musical notes. The proposed system was implemented using MATLAB R2016a, Work was done on a Windows10 Processor OS (Intel ® Core TM i5-7200U CPU @ 2.50GHZ 2.70GHZ) computer.},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\PUBRZKLR\Younis and Ramo - 2021 - A new parallel bat algorithm for musical note reco.pdf}
}
